{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ncf-isp2020-nlmlp-day3-student.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iMVVmuc3mR7"
      },
      "source": [
        "%%time\n",
        "# Download spaCy model with word embeddings\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG6A2L7-jUwB"
      },
      "source": [
        "# Data Preparation\n",
        "\n",
        "Clone GitHub repository to Colab storage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXLCNpOZiyAs"
      },
      "source": [
        "!git clone https://github.com/megagonlabs/HappyDB.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUYf9g9_jFKH"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WasmXyADjHT7"
      },
      "source": [
        "!ls HappyDB/happydb/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjO7kzYIfWun"
      },
      "source": [
        "# Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fixxp-40pQ3p"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.base import clone\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def run_cv(X, y, clf, num_classes, n_splits=5):\n",
        "  kf = KFold(n_splits=n_splits, random_state=1)\n",
        "  cm = np.zeros([num_classes,\n",
        "                  num_classes],\n",
        "                  dtype=\"int\") # Initialize confusion matrix with 0\n",
        "  f1_list = []\n",
        "  for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
        "    print(\"Fold {}\".format(i + 1))\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    cur_clf = clone(clf)\n",
        "    cur_clf.fit(X_train, y_train)\n",
        "    y_pred = cur_clf.predict(X_test)\n",
        "    cm += confusion_matrix(y_test, y_pred)\n",
        "    f1_list.append(f1_score(y_test, y_pred, average=\"macro\"))\n",
        "  f1_scores = np.array(f1_list)\n",
        "  return (f1_scores, cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsnWtFElj3_v"
      },
      "source": [
        "## Loading CSV file as DataFrame\n",
        "\n",
        "Use `.read_csv()` function to load a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFwKyYpYigFM"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Kijkbc5i9ap"
      },
      "source": [
        "hm_df = pd.read_csv(\"HappyDB/happydb/data/cleaned_hm.csv\")\n",
        "hm_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O3TGhi0kLA0"
      },
      "source": [
        "# Filtering out samples that do not have ground truth labels\n",
        "#   or # of sentences > 3\n",
        "filtered_hm_df = hm_df[(hm_df[\"num_sentence\"] <= 3) &\n",
        "                       (~ hm_df[\"ground_truth_category\"].isnull())]\n",
        "                       \n",
        "print(\"Original # of HM: {}\".format(len(hm_df)))\n",
        "print(\"Filtered # of HM: {}\".format(len(filtered_hm_df)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kt69LV3HlsAe"
      },
      "source": [
        "# Label vector & Feature matrix creation\n",
        "\n",
        "Let's create label vector and feature matrix from the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-dsqTP0mGC2"
      },
      "source": [
        "# Label Encoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(filtered_hm_df[\"ground_truth_category\"])\n",
        "y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAH1JBYWmyKX"
      },
      "source": [
        "le.classes_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jkSMjEXC_g-"
      },
      "source": [
        "Xcount = CountVectorizer().fit_transform(filtered_hm_df[\"cleaned_hm\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zih8QW8a0BtU"
      },
      "source": [
        "# Word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66donlzB0DZm"
      },
      "source": [
        "## You need to restart the runtime to use spaCy in the usual style.\n",
        "# import spacy\n",
        "# nlp = spacy.load(\"en_core_web_lg\")  # \"en_core_web_lg\" does not provice embeddings\n",
        "## Use the following style instead.\n",
        "import en_core_web_lg\n",
        "nlp = en_core_web_lg.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZphS2FY5TW_"
      },
      "source": [
        "# Sample code\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion. jkdsjaflksj\")\n",
        "info_list = []\n",
        "for token in doc:\n",
        "    info_list.append([token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "            token.shape_, token.is_alpha, token.is_stop,\n",
        "            token.vector_norm, token.is_oov])\n",
        "pd.DataFrame(\n",
        "    info_list, columns=[\"TEXT\", \"LEMMA\", \"POS\", \"TAG\", \"DEP\", \"SHAPE\", \"ALPHA\", \"STOP\",\n",
        "                        \"VECTOR_NORM\", \"OOV\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqekB1RvMAgh"
      },
      "source": [
        "## Visualize word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvtvS6bZJad-"
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "import seaborn as sns\n",
        "\n",
        "## TRY! Change the following line and see how the word embeddings look like\n",
        "words = [\"carmine\", \"red\", \"purple\", \"orange\", \"green\", \"white\", \"cat\", \"dog\"]\n",
        "## ============================================================================\n",
        "\n",
        "wvecs = np.array([nlp(w).vector for w in words])\n",
        "wvecs_2d = TruncatedSVD(n_components=2).fit_transform(wvecs)\n",
        "\n",
        "# Visualize plots\n",
        "ax = sns.scatterplot(wvecs_2d[:, 0], wvecs_2d[:, 1])\n",
        "for i, w in enumerate(words):\n",
        "  ax.text(wvecs_2d[i, 0] + 0.1, wvecs_2d[i, 1] + 0.1, w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C22wXvYpMMBZ"
      },
      "source": [
        "## Cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmpLQ256IAVc"
      },
      "source": [
        "# Word embeddings\n",
        "from scipy.spatial.distance import cosine\n",
        "def cossim(x, y):\n",
        "  return 1.0 - cosine(x, y)\n",
        "\n",
        "for w1, w2 in [(\"carmine\", \"red\"),\n",
        "               (\"carmine\", \"purple\"),\n",
        "               (\"carmine\", \"orange\"),\n",
        "               (\"carmine\", \"green\"),\n",
        "               (\"carmine\", \"white\"),\n",
        "               (\"carmine\", \"cat\")]:\n",
        "  print(\"cossim(\\\"{}\\\",\\\"{}\\\")={:.4f}\".format(w1, w2,cossim(nlp(w1).vector, nlp(w2).vector)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbPqm9mPMEs7"
      },
      "source": [
        "# Use sentence embeddings as features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAXXFvrw6pwB"
      },
      "source": [
        "def sent2vec(nlp, s):\n",
        "  \"\"\"Converts a sentence into a vector representation.\"\"\"\n",
        "  wvec_list = []\n",
        "  for token in nlp(s):\n",
        "    # Skip OOV words (= zero vector)\n",
        "    if token.is_oov:\n",
        "      continue\n",
        "    wvec_list.append(token.vector)\n",
        "  wvecs = np.array(wvec_list)\n",
        "  return wvecs.mean(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrGKTP2h7T1B"
      },
      "source": [
        "# Takes about 2 minutes\n",
        "%%time\n",
        "Xsentvec = np.array(\n",
        "        filtered_hm_df[\"cleaned_hm\"].apply(lambda x: sent2vec(nlp, x)).tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaSNosohEXOg"
      },
      "source": [
        "# Try other feature extraction methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIGmCFCMf2RL"
      },
      "source": [
        "%%time\n",
        "f1_scores_count_lr, _ = run_cv(Xcount, y, LogisticRegression(), len(le.classes_))\n",
        "f1_scores_sentvec_lr, _ = run_cv(Xsentvec, y, LogisticRegression(), len(le.classes_))\n",
        "\n",
        "## [Optional] Uncomment below\n",
        "#f1_scores_count_gbt, _ = run_cv(Xcount, y, GradientBoostingClassifier(), len(le.classes_))\n",
        "#f1_scores_sentvec_gbt, _ = run_cv(Xsentvec, y, GradientBoostingClassifier(), len(le.classes_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrYVj7ipgJGf"
      },
      "source": [
        "eval_df = pd.DataFrame({\"LR+CountVec\": f1_scores_count_lr,\n",
        "                        \"LR+Sent2vec\": f1_scores_sentvec_lr})\n",
        "\n",
        "## [Optional] Use the code below if you also run GBT\n",
        "\"\"\"\n",
        "eval_df = pd.DataFrame({\"LR+CountVec\": f1_scores_count_lr,\n",
        "                        \"LR+Sent2vec\": f1_scores_sentvec_lr,\n",
        "                        \"GBT+CountVec\": f1_scores_count_gbt,\n",
        "                        \"GBT+Sent2vec\": f1_scores_sentvec_gbt})\n",
        "\"\"\"\n",
        "eval_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yffxGNARGw17"
      },
      "source": [
        "eval_df.mean(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RdMQeclGges"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG8MeDVSGhn1"
      },
      "source": [
        "# [Advanced] Fine-tuning BERT for happiness category classification\n",
        "\n",
        "Fine-tune a BERT model for the same task. `transformers` library by Huggingface is the most common and easy-to-use Python library. \n",
        "\n",
        "https://github.com/huggingface/transformers\n"
      ]
    }
  ]
}